export const initialPlotText: string[] = [
  'Logistic regression is, at the heart of it, a means of classification. In this example we are attempting to predict whether a house is located in either New York (blue data points) or San Francisco (green data points) given other qualifying data such as elevation, price per square foot, or number of bedrooms. In the figure on the left individual houses are plotted by their elevation - it is relatively easy to see that if a house is located higher than 73 meters above sea level it is almost certainly in San Francisco.',
  'In this instance, we are using elevation as a feature. If we add more features, to our model then we are able to generate better predictions (within reason - including too many features may lead to problems of overfitting or confounding variables among other issues).',
  'Once we add another feature, in this case price per square foot, it is easy to see how many of the houses in New York and San Francisco begin to separate even more.',
  "At this point, given the two features it is fairly easy to see that any houses in the blue box are houses in New York, while any houses in the green box are in San Francisco. The goal of logistic regression is to predict those houses programatically. To do this we want to build a model that can give us the probability of a house belonging to a particular location. To begin talking about how to do that, let's go back to just looking at elevation data."
]

export const radialPlotText: string[] = [
  'Since logistic regression is modelling probability, we want to use a model that, given any input, will output a prediction between 0 and 1. This model uses the sigmoid function to classify where a house belongs, and in this instance is able to learn by iteratively taking running a training set of data (think array of elevation values) through the model and subsequently minimizing the values of our loss function. The loss function that is commonly used is called the cross entropy equation - there are many, many, many resources out there that will explain these concepts much better than I can so I will leave it to the reader to delve deeper.',
  'The main gist is that once we take the derivative of our loss function we get the slope of of our error - basically how far off our guesses were given the training data and an initially arbitrary set of weights (i.e. in the y = mX + b equation m and b are our weights). When we take the slope and multiply it by a learning rate (equally arbitrary) then we can then take that value and subtract it from our weights, thus updating the wieghts in a direction that serves to minimize the differences between our guess and the true value. Using those weights we are able to plug them back into the sigmoid equation and generate a line, which we would subsequently use to make a set of predictions again and repeat the process.',
  'This process, in which we use gradient descent to iteratively generate new weights that minimize the incorrectly predicted houses in our training set of data, allows us to slowly approach an optimal logistic model for predicting housing location. This is shown by the red line to the left, whereas the grey lines represent previous iterations of the training process.',
  'Since iteratively training our model can be computationally expensive, it is worth looking at the loss value (generated by the cross entropy equation) plotted against iterations - after a while it is possible to see that the function begins to level off and there are diminishing returns to running more iterations. At some point, the model is "good enough" and can be tested on a set of data specifically withheld from the training data with the intention of providing some metrics on how accurate our model actually is at predicting housing locations based off data it has never seen before.',
  'In reality, there are many decisions to be made when evaluating how to train a model - everything from how much data to use to batching data to adjusting the learning rate - and each decision will effect both the accuracy and computational cost of training the model. Although this explanation glossed over many of those decisions, the following interactive graphics will allow you to testing out training your own model using different features and training parameters, and a link will be provided afterwords with more resources.'
]